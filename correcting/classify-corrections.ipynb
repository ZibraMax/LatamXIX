{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Manual-code categorization of surface-forms and errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the corrections made by the LLM in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from difflib import SequenceMatcher\n",
    "punctuation += \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ([], [])\n",
      "2 (['aa'], ['hf'])\n",
      "0 (['aa'], ['ha'])\n"
     ]
    }
   ],
   "source": [
    "def normalize(c): return unicodedata.normalize('NFKD', c).encode('ASCII', 'ignore').decode()\n",
    "\n",
    "def useful_chars(string):\n",
    "    return re.sub(r'[^a-zA-ZÀ-ÿ]', '', string)\n",
    "\n",
    "def count_accent_changes(str1, str2):\n",
    "    if len(str1) != len(str2):\n",
    "        return -1\n",
    "\n",
    "    changes = 0\n",
    "    for char1, char2 in zip(str1, str2):\n",
    "        if char1 != char2 and normalize(char1) == normalize(char2):\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "def categorize_char(c):\n",
    "    if c.isdigit():\n",
    "        return 'd'\n",
    "    elif c.isalpha():\n",
    "        return 'a'\n",
    "    elif c in punctuation:\n",
    "        return 's'\n",
    "    else:\n",
    "        return 'u'\n",
    "\n",
    "def nonaccent_changes(str1, str2):\n",
    "    #str1 = useful_chars(str1)\n",
    "    #str2 = useful_chars(str2)\n",
    "    if len(str1) != len(str2):\n",
    "        return [], []\n",
    "\n",
    "    typechanges = [] # d (digit), a (alpha character), s (special), u (unknown). All changes are a couple types such as \"dc\" (digit changed to consonant)\n",
    "    changes = []\n",
    "    for char1, char2 in zip(str1, str2):\n",
    "        nchar1 = normalize(char1)\n",
    "        nchar2 = normalize(char2)\n",
    "        if char1 != char2 and nchar1 != nchar2:\n",
    "            typechanges.append(categorize_char(char1) + categorize_char(char2))\n",
    "            changes.append(char1+char2)\n",
    "\n",
    "    return typechanges, changes\n",
    "\n",
    "print(count_accent_changes(\"holá\", \"hola\"), nonaccent_changes(\"holá\", \"hola\"))   # Output: 1 (á -> a)\n",
    "print(count_accent_changes(\"feñhá\", \"fenfa\"), nonaccent_changes(\"feñhá\", \"fenfa\")) # Output: 2 (ñ -> n, á -> a) the rest of the changes are not due to the accent\n",
    "print(count_accent_changes(\"olh\", \"ola\"), nonaccent_changes(\"olh\", \"ola\"))     # Output: 0 (no accent changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Go back to this point to reset all the applied categorizations and start over the debugging process, when required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTIONS_FILE = \"./correctionsLatam.json\"\n",
    "SURFACE_FORMS_FILE = \"../data/surfaceForms.json\"\n",
    "SURFACE_FORMS_FILE_NONACC = \"../data/surfaceFormsNonAccents.json\"\n",
    "ORTHOGRAPHIC_ERRORS_FILE = \"../data/orthographicErrors.json\"\n",
    "COLOR_PRINTING = False # for large corpus, set it to False\n",
    "current_step = 0\n",
    "\n",
    "df = pd.read_parquet(\"../data/pre-corrected-latam-xix.parquet\")\n",
    "\n",
    "with open(CORRECTIONS_FILE, 'r') as infile:\n",
    "    fixes = json.load(infile)\n",
    "\n",
    "if not os.path.exists(SURFACE_FORMS_FILE):\n",
    "    with open(SURFACE_FORMS_FILE, 'w') as outfile:\n",
    "        json.dump({}, outfile)\n",
    "surface_forms = dict()\n",
    "\n",
    "if not os.path.exists(SURFACE_FORMS_FILE_NONACC):\n",
    "    with open(SURFACE_FORMS_FILE_NONACC, 'w') as outfile:\n",
    "        json.dump({}, outfile)\n",
    "surface_forms_nacc = dict()\n",
    "\n",
    "if not os.path.exists(ORTHOGRAPHIC_ERRORS_FILE):\n",
    "    with open(ORTHOGRAPHIC_ERRORS_FILE, 'w') as outfile:\n",
    "        json.dump([], outfile)\n",
    "orthographic_errors = dict() # will be changed to list before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La publicacion del Oso se harà dos veces cada semana, y constará de un pliego en cuarto ; ofreciendo à mas sus redactores, dar los gravados oportunos, siempre que lo exija el asunto de que trate. Redactado por un Num. 8. TEMA del Periodico. POLITICA MILITAR. OCTAVA SESION. Abierta la sesion á las dore y un minuto de la noche , 25 de Febrero de 1845 , con asistencia de todos los Señores Representantes, se leyó y aprobó la acta de la Asamblea anterior , ménos en lo tocante à la torre del Convento de Santo Domingo, punto que quedó para ventilarse en mejor ocasion. Enseguida se dió cuenta de una nota del Ejecutivo , referente à que urjía la necesidad de organizar un Ejército ; pues decia el Excmo. Decano: - \"Un poder sin bayonetas vale tanto como un cero puesto á la izquierda.\"'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114411 fixes to check\n",
      "0 surface forms found\n",
      "0 orthographic errors found\n"
     ]
    }
   ],
   "source": [
    "def add_to_surface(wrong, good, freq):\n",
    "    sf_t, real_t = wrong.lower(), good.lower()\n",
    "    sf_ws, real_ws = re.findall(r'\\w+', sf_t), re.findall(r'\\w+', real_t)\n",
    "\n",
    "    if len(sf_ws) == len(real_ws):\n",
    "        for sf,real in zip(sf_ws, real_ws):\n",
    "            if sf != real:\n",
    "                numaccentchanges = count_accent_changes(real, sf)\n",
    "                surface_forms[real] = surface_forms.get(real, dict())\n",
    "                surface_forms[real][sf] = surface_forms[real].get(sf, 0) + freq\n",
    "                if numaccentchanges < 1:\n",
    "                    surface_forms_nacc[real] = surface_forms_nacc.get(real, dict())\n",
    "                    surface_forms_nacc[real][sf] = surface_forms_nacc[real].get(sf, 0) + freq\n",
    "\n",
    "    else:\n",
    "        if sf_t != real_t:\n",
    "            numaccentchanges = count_accent_changes(real_t, sf_t)\n",
    "            surface_forms[real_t] = surface_forms.get(real_t, dict())\n",
    "            surface_forms[real_t][sf_t] = surface_forms[real_t].get(sf_t, 0) + freq\n",
    "            if numaccentchanges < 1:\n",
    "                surface_forms_nacc[real_t] = surface_forms_nacc.get(real_t, dict())\n",
    "                surface_forms_nacc[real_t][sf_t] = surface_forms_nacc[real_t].get(sf_t, 0) + freq\n",
    "\n",
    "def add_to_errors(fix):\n",
    "    for idx,widx1,widx2,ctx in fix['usages']:\n",
    "        orthographic_errors[(idx,widx1,widx2)] = {\n",
    "            \"prv\": fix['change'][0],\n",
    "            \"mod\": fix['change'][1],\n",
    "            \"ctx\": ctx\n",
    "        }\n",
    "\n",
    "def print_fix(wrong, good, freq, category):\n",
    "    if COLOR_PRINTING:\n",
    "        if category == \"SFRM\": \n",
    "            category = f\"[\\033[93m{category}\"\n",
    "        elif category == \"ERRR\":\n",
    "            category = f\"[\\033[95m{category}\"\n",
    "        else:\n",
    "            category = f\"[\\033[90m{category}\"\n",
    "        print(f\"{category}\\033[0m] (\\033[92m{freq}\\033[0m) \\033[91m{wrong}\\033[0m - \\033[94m{good}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"[{category}] ({freq}) {wrong} - {good}\")\n",
    "\n",
    "def execute_prev_steps(step):\n",
    "    if step <= current_step + 1:\n",
    "        return\n",
    "    elif step - 2 > current_step:\n",
    "        raise Exception(f\"Execute step {step-2} first\")\n",
    "    else:\n",
    "        if step >= 2:\n",
    "            i = step-1\n",
    "            print(f\"Executing step {i}...\")\n",
    "            globals()[f\"step{i}\"](False)\n",
    "            status()\n",
    "\n",
    "status = lambda: print(f\"{len(fixes)} fixes to check\\n{len(surface_forms)} surface forms found\\n{len(orthographic_errors)} orthographic errors found\")\n",
    "status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually-written steps for categorization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "\n",
    "- Each correction made by the LLM may be categorized as:\n",
    "  * **surface form**: with the method `add_to_surface`\n",
    "  * **orthographic error**: with the method `add_to_errors`\n",
    "  * **none**: just `pass` (skip) the correction\n",
    "\n",
    "- Each step has a `JUST_PRINT` variable; if it's True, the changes won't affect the variables. For debugging a particular step:\n",
    "  * First, **ALWAYS run all the previous steps with the `JUST_PRINT` variable set to False**\n",
    "  * Then, set the `JUST_PRINT` variable of the step you want to debug to True and run the step\n",
    "  * If you run the next step without running the previous one (with `JUST_PRINT`=False), it'll run the previous automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ALL texts that are the same except for characters with an accent, such as \"barbaros\" -> \"bárbaros\"\n",
    "**surface forms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99392 fixes to check\n",
      "9486 surface forms found\n",
      "0 orthographic errors found\n"
     ]
    }
   ],
   "source": [
    "JUST_PRINT = False\n",
    "\n",
    "def step1(just_print):\n",
    "    global current_step\n",
    "    execute_prev_steps(1)\n",
    "    if just_print == False: current_step = 1\n",
    "\n",
    "    idx_remove = []\n",
    "    for i,fix in enumerate(fixes):\n",
    "        numaccentchanges = count_accent_changes(fix['change'][0], fix['change'][1])\n",
    "        _, changes = nonaccent_changes(fix['change'][0].lower(), fix['change'][1].lower())\n",
    "        if ( True\n",
    "            ) and (len(re.findall(r'\\w+', fix['change'][0])) == len(re.findall(r'\\w+', fix['change'][1])) # have the same length\n",
    "            ) and (numaccentchanges >= 1 # has any different accent letter\n",
    "            ) and (len(changes) == 0 # is the same word\n",
    "        ):\n",
    "            for s,c in zip(re.findall(r'\\w+', fix['change'][0]), re.findall(r'\\w+', fix['change'][1])):\n",
    "                if (len(s) == len(c) != 0) and (s != c) and (s not in punctuation):\n",
    "                    if just_print: print_fix(*fix['change'], fix['freq'], \"SFRM\")\n",
    "                    else: add_to_surface(*fix['change'], fix['freq'])\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    if just_print: print(len(idx_remove))\n",
    "    else: \n",
    "        for i in reversed(idx_remove): fixes.pop(i)\n",
    "\n",
    "step1(JUST_PRINT)\n",
    "status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SPECIFIC type of changes\n",
    "**surface forms or orthographic errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94847 fixes to check\n",
      "11329 surface forms found\n",
      "374 orthographic errors found\n"
     ]
    }
   ],
   "source": [
    "JUST_PRINT = False\n",
    "\n",
    "def step2(just_print):\n",
    "    global current_step\n",
    "    execute_prev_steps(2)\n",
    "    if just_print == False: current_step = 2\n",
    "\n",
    "    idx_remove = []\n",
    "    sforms_specifications = ['iy', 'jg', 'sx', 'xj', 'vb', 'bv', 'sz', 'zc', 'zs', 'sx', 'yi', 'cs', 'qc', 'gj', 'xs', 'sc', 'jx']\n",
    "    errors_specifications = ['6ó', '1y', '0o', '4a']\n",
    "    specifications = sforms_specifications+errors_specifications\n",
    "\n",
    "    for i,fix in enumerate(fixes):\n",
    "        _, changes = nonaccent_changes(fix['change'][0].lower(), fix['change'][1].lower())\n",
    "        if ( True\n",
    "            ) and (len(re.findall(r'\\w+', fix['change'][0])) == len(re.findall(r'\\w+', fix['change'][1])) # have the same length\n",
    "            ) and (any(s in changes for s in specifications) # has any change in the list such as 'soi' -> 'soy'\n",
    "        ):\n",
    "            for s,c in zip(re.findall(r'\\w+', fix['change'][0]), re.findall(r'\\w+', fix['change'][1])):\n",
    "                if (len(s) == len(c) != 0) and (s != c) and (s not in punctuation):\n",
    "                    tc, ac = nonaccent_changes(s.lower(),c.lower())\n",
    "                    if all([k in sforms_specifications for k in ac]):\n",
    "                        if just_print: print_fix(s, c, fix['freq'], \"SFRM\")\n",
    "                        else: add_to_surface(s,c,fix['freq'])\n",
    "                    elif all([k in errors_specifications for k in ac]):\n",
    "                        if just_print: print_fix(s, c, fix['freq'], \"ERRR\")\n",
    "                        else: add_to_errors(fix)\n",
    "                    else:\n",
    "                        if just_print: print_fix(*fix['change'], fix['freq'], \"NONE\")\n",
    "                        else: pass\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    if just_print: print(len(idx_remove))\n",
    "    else: \n",
    "        for i in reversed(idx_remove): fixes.pop(i)\n",
    "\n",
    "step2(JUST_PRINT)\n",
    "status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ALL texts where there are letter-letter changes, and have same length\n",
    "**orthographic errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67246 fixes to check\n",
      "11329 surface forms found\n",
      "39454 orthographic errors found\n"
     ]
    }
   ],
   "source": [
    "JUST_PRINT = False\n",
    "\n",
    "def step3(just_print):\n",
    "    global current_step\n",
    "    execute_prev_steps(3)\n",
    "    if just_print == False: current_step = 3\n",
    "\n",
    "    idx_remove = []\n",
    "    nonconsider = [\"sugestiones\", \"suerte\"] # neither surface-forms nor orthographic-errors \n",
    "    exceptions = [\"Presidenta\"]\n",
    "\n",
    "    for i,fix in enumerate(fixes):\n",
    "        typechanges, _ = nonaccent_changes(fix['change'][0].lower(), fix['change'][1].lower())\n",
    "        if fix['change'][0] in nonconsider:\n",
    "            if just_print: print_fix(*fix['change'], fix['freq'], \"NONE\")\n",
    "            else: pass\n",
    "        elif fix['change'][0] in exceptions:\n",
    "            if just_print: print_fix(*fix['change'], fix['freq'], \"SFRM\")\n",
    "            else: add_to_surface(*fix['change'], fix['freq'])\n",
    "        elif ( True\n",
    "            ) and (len(re.findall(r'\\w+', fix['change'][0])) == len(re.findall(r'\\w+', fix['change'][1])) # have the same length\n",
    "            ) and ('aa' in typechanges # has a letter-letter change\n",
    "        ):\n",
    "            if just_print: print_fix(*fix['change'], fix['freq'], \"ERRR\")\n",
    "            else: add_to_errors(fix)\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    if just_print: print(len(idx_remove))\n",
    "    else: \n",
    "        for i in reversed(idx_remove): fixes.pop(i)\n",
    "\n",
    "step3(JUST_PRINT)\n",
    "status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The remaining changes\n",
    "\n",
    "Finally, the remaining rows may be very different. A good way to know if the corrections are mostly truthy, it's possible to compute a similarity ratio between two texts, using difflib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity(text1, text2):\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "similarity(\"que\", \"como\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ALL texts where there are non letter-letter changes, or have different length\n",
    "**orthographic errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fixes to check\n",
      "11361 surface forms found\n",
      "96912 orthographic errors found\n"
     ]
    }
   ],
   "source": [
    "JUST_PRINT = False\n",
    "MIN_SIMILARITY = .55\n",
    "\n",
    "def step4(just_print):\n",
    "    global current_step\n",
    "    execute_prev_steps(4)\n",
    "    if just_print == False: current_step = 4\n",
    "\n",
    "    idx_remove = []\n",
    "    nonconsider = []\n",
    "    exceptions = ['trasformación', 'suscriciones', 'trasportar', 'trasmitir', 'trascurso', 'trasparente', \n",
    "                'trasporte', 'trascurrido', 'Setiembre', 'trasparenta', 'Hispano-America', 'Dirigióse', \n",
    "                'Viólo', 'Costarrica', 'COSTARRICA', 'distinguióse', 'comprofesores', 'diez y seis',\n",
    "                'bien que', 'de el', 'D.', 'suscritores', \"q'\", 'usté', 'incorporóse', 'estraños', 'chicuelos',\n",
    "                'trascurría', 'trasmita', 'Ud.', 'Sr.', 'trasportes', 'trasmisión', 'libertar', 'nikel', 'kioskos',\n",
    "                'apuntaciones', 'trasformaciones', 'boulevares'] # TODO: continue...\n",
    "\n",
    "    for i,fix in enumerate(fixes):\n",
    "        if fix['change'][0] in nonconsider:\n",
    "            if just_print: print_fix(*fix['change'], fix['freq'], \"NONE\")\n",
    "            else: pass\n",
    "        elif fix['change'][0] in exceptions:\n",
    "            if just_print: print_fix(*fix['change'], fix['freq'], \"SFRM\")\n",
    "            else: add_to_surface(*fix['change'], fix['freq'])\n",
    "        else:\n",
    "            sim = similarity(fix['change'][0].lower(), fix['change'][1].lower())\n",
    "            if ( True\n",
    "                and (\n",
    "                       (len(re.findall(r'\\w+', fix['change'][0])) > 5) \n",
    "                    or (len(re.findall(r'\\w+', fix['change'][1])) > 5) \n",
    "                    or  sim < MIN_SIMILARITY \n",
    "                    )\n",
    "                and (fix['freq'] <= 3)\n",
    "               ):\n",
    "                if just_print: print_fix(*fix['change'], fix['freq'], \"NONE\")\n",
    "                else: pass\n",
    "            else:\n",
    "                if just_print: print_fix(*fix['change'], fix['freq'], \"ERRR\")\n",
    "                else: add_to_errors(fix)\n",
    "        idx_remove.append(i)\n",
    "\n",
    "    if just_print: print(len(idx_remove))\n",
    "    else: \n",
    "        for i in reversed(idx_remove): fixes.pop(i)\n",
    "\n",
    "step4(JUST_PRINT)\n",
    "status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_forms = {k: v for k,v in sorted(surface_forms.items(), key=lambda x: sum(x[1].values()), reverse=True)}\n",
    "surface_forms_nacc = {k: v for k,v in sorted(surface_forms_nacc.items(), key=lambda x: sum(x[1].values()), reverse=True)}\n",
    "\n",
    "with open(SURFACE_FORMS_FILE_NONACC, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(surface_forms_nacc, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(SURFACE_FORMS_FILE, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(surface_forms, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prv': 'à mas',\n",
       "  'mod': 'además',\n",
       "  'ctx': 'pliego en cuarto ; ofreciendo à mas sus redactores , dar los',\n",
       "  'idx1': 101,\n",
       "  'idx2': 106},\n",
       " {'prv': 'dore',\n",
       "  'mod': 'dos',\n",
       "  'ctx': 'Abierta la sesion á las dore y un minuto de la',\n",
       "  'idx1': 298,\n",
       "  'idx2': 302}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthographic_errors = dict(sorted(orthographic_errors.items(), key=lambda item: (item[0][0], item[0][1])))\n",
    "\n",
    "errors = [[] for _ in range(len(df))]\n",
    "for k,v in orthographic_errors.items():\n",
    "    v['idx1'] = k[1]\n",
    "    v['idx2'] = k[2]\n",
    "    errors[k[0]].append(v)\n",
    "\n",
    "with open(ORTHOGRAPHIC_ERRORS_FILE, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(errors, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "errors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La publicacion del Oso se harà dos veces cada semana, y constará de un pliego en cuarto ; ofreciendo à mas sus redactores, dar los gravados oportunos, siempre que lo exija el asunto de que trate. Redactado por un Num. 8. TEMA del Periodico. POLITICA MILITAR. OCTAVA SESION. Abierta la sesion á las dore y un minuto de la noche , 25 de Febrero de 1845 , con asistencia de todos los Señores Representantes, se leyó y aprobó la acta de la Asamblea anterior , ménos en lo tocante à la torre del Convento de Santo Domingo, punto que quedó para ventilarse en mejor ocasion. Enseguida se dió cuenta de una nota del Ejecutivo , referente à que urjía la necesidad de organizar un Ejército ; pues decia el Excmo. Decano: - \"Un poder sin bayonetas vale tanto como un cero puesto á la izquierda.\"'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(errors):\n",
    "    text = df.loc[i, \"text\"]\n",
    "    chdif = 0\n",
    "    for v in e:\n",
    "        p = v['prv']\n",
    "        m = v['mod']\n",
    "        ctx = v['ctx']\n",
    "        idx1 = v['idx1'] - chdif\n",
    "        idx2 = v['idx2'] - chdif\n",
    "        assert p == text[idx1:idx2], f\"ERROR at {i}. Expected {p} but got {text[idx1:idx2]}\"\n",
    "        text = text[:idx1] + m + text[idx2:]\n",
    "        chdif += len(p) - len(m)\n",
    "    df.loc[i, \"text\"] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La publicacion del Oso se harà dos veces cada semana, y constará de un pliego en cuarto ; ofreciendo además sus redactores, dar los gravados oportunos, siempre que lo exija el asunto de que trate. Redactado por un Num. 8. TEMA del Periodico. POLITICA MILITAR. OCTAVA SESION. Abierta la sesion á las dos y un minuto de la noche , 25 de Febrero de 1845 , con asistencia de todos los Señores Representantes, se leyó y aprobó la acta de la Asamblea anterior , ménos en lo tocante à la torre del Convento de Santo Domingo, punto que quedó para ventilarse en mejor ocasion. Enseguida se dió cuenta de una nota del Ejecutivo , referente à que urjía la necesidad de organizar un Ejército ; pues decia el Excmo. Decano: - \"Un poder sin bayonetas vale tanto como un cero puesto á la izquierda.\"'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/corrected-latam-xix.tsv\", sep=\"\\t\", index=False)\n",
    "df.to_parquet('../data/corrected-latam-xix.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
